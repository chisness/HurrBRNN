{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-8b37332056c9>, line 79)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-8b37332056c9>\"\u001b[0;36m, line \u001b[0;32m79\u001b[0m\n\u001b[0;31m    epsilon_t = = pm.Normal('epsilon_t', sd=sigma_e2) #doesn't need the identity matrix since output is just a vector?\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pymc3 as pm\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "#Implementing BAST-RNN from https://arxiv.org/pdf/1711.00636.pdf\n",
    "#Priors are all given on page 40\n",
    "#Full conditionals are given on pages 45-46\n",
    "\n",
    "#Questions\n",
    "#1) Dimension reduction on pages 12-13?\n",
    "#2) What should the embedding lag and length be for the input vector on page 9? \n",
    "#3) \n",
    "\n",
    "#Implementation\n",
    "#1) Define priors and dimensions (page 40)\n",
    "#2) Run PX-MCMC from Alg 1 (page 44) from paper sampling 100,000 iterations with the first 25,000 as burn-in, \n",
    "#thinning the samples so every fifth sample post burn-in is kept (keep 15,000). Monitor convergence with trace plots of \n",
    "#parameters and posterior forecasts. (Details on pages 40-46)\n",
    "#Note that in PX-MCMC: \n",
    "#Step 1 is bullet point 1 in Appendix C (sampled with Metropolis-Hastings steps)\n",
    "#Steps 2-4 are bullet point 2 in Appendix C (sampled with Metropolis-Hastings steps)\n",
    "#Step 5 represents bullets 3-8 in Appendix C (samples with Gibbs steps)\n",
    "\n",
    "\n",
    "#HIDDEN h_t = f(delta/abs(lambda_w)*W_(h_(t-1)) + U*X~_t) from equation (4) on page 8\n",
    "print('HIDDEN')\n",
    "\n",
    "#DIMENSIONS AND DETAILS\n",
    "#f is activation function, use tanh np.tanh()\n",
    "#h_t: n_h x n_h hidden layer\n",
    "#n_h = 20 (given number of hidden units in paper)\n",
    "#w_i,l: i = 1,...,n_h ; l = 1,...,n_h | square n_h x n_h matrix \n",
    "#u_i,r: i = 1,...,n_h ; r = 1,...,(m+1)*n_x + 1\n",
    "#delta scaling parameter with Unif(0,1) prior\n",
    "#lambda_w is largest eigenvalue of matrix W\n",
    "#h_0 = 0 by definition\n",
    "#X~_t from equation (5) on page 9 is a vector of [X_t, X_(t-tau), ..., X_(t-m*tau)]\n",
    "#alpha is expansion parameter with dimensions i,l both of length n_h\n",
    "\n",
    "#PRIORS\n",
    "a_w = 0.2 #truncation of normal\n",
    "a_u = 0.2\n",
    "with pm.Model() as model:\n",
    "    delta = pm.Uniform('delta', lower=0, upper=1)\n",
    "    \n",
    "    gammaw_il = pm.Bernoulli('gammaw_il', p=0.2)\n",
    "    tnw1 = pm.TruncatedNormal('tnw1', mu=0, sd=1000, lower=-a_w, upper=a_w)\n",
    "    tnw2 = pm.TruncatedNormal('tnw2', mu=0, sd=math.sqrt(.001), lower=-a_w, upper=a_w) #.001 or sqrt(.001)?\n",
    "    w = gammaw_il*tnw1 + (1-gammaw_il)*tnw2 # should be matrix i x l \n",
    "    \n",
    "    gammau_ir = pm.Bernoulli('gammau_ir', p=.025)\n",
    "    tnu1 = pm.TruncatedNormal('tnu1', mu=0, sd=1000, lower=-a_u, upper=a_u)\n",
    "    tnu2 = pm.TruncatedNormal('tnu2', mu=0, sd=math.sqrt(.0005), lower=-a_u, upper=a_u) #.0005 or sqrt(.0005)?\n",
    "    u = gammau_ir*tnu1 + (1-gammaw_il)*tnu2 #should be matrix i x r\n",
    "\n",
    "    \n",
    "#DATA Y_t = mu + V_1*h_t + V_2*h_t^2 + epsilon_t from equation (3) on page 8\n",
    "print('\\n\\nDATA')\n",
    "\n",
    "#DIMENSIONS AND DETAILS\n",
    "#mu ~ Gau(0, (sigma_mu)^2 * I \n",
    "#: n_y | \n",
    "#V_1: n_y x n_h\n",
    "#V_2: n_y x n_h\n",
    "#epsilon_t ~ Gau(0,R_t)\n",
    "#R_t := (sigma_epsilon)^2 * I \n",
    "#(sigma_epsilon)^2 ~ IG(alpha_epsilon, beta_epsilon) \n",
    "#the above is inverse gamma with priors alpha_epsilon = .001 and beta_epsilon = .001\n",
    "\n",
    "#output_size = 100\n",
    "\n",
    "#PRIORS\n",
    "with pm.Model() as model:\n",
    "    sigma_e2 = pm.InverseGamma('sigma_e2', alpha=.001, beta=.001)\n",
    "    mu = pm.Normal('mu', sd = 10) #doesn't need the identity matrix since output is just vector?\n",
    "    epsilon_t = = pm.Normal('epsilon_t', sd=sigma_e2) #doesn't need the identity matrix since output is just a vector?\n",
    "    \n",
    "    gammav1_ki = pm.Bernoulli('gammav1_ki', p=0.5)\n",
    "    v1ki1 = pm.Normal('v1ki1', mu=0, sd=math.sqrt(10)) #10 or sqrt(10)?\n",
    "    v1ki1 = pm.Normal('tnw2', mu=0, sd=math.sqrt(.01)) #.01 or sqrt(.01)?\n",
    "    v1 = gammav1_ki*v1ki1 + (1-gammaw_il)*v1ki1 # should be matrix y x h\n",
    "    \n",
    "    gammav2_ki = pm.Bernoulli('gammav2_ki', p=.25)\n",
    "    v2ki1 = pm.Normal('v2ki1', mu=0, sd=math.sqrt(0.5), lower=-a_u, upper=a_u) #.5 or sqrt(.5)?\n",
    "    v2ki2 = pm.Normal('v2ki2', mu=0, sd=math.sqrt(.05), lower=-a_u, upper=a_u) #.05 or sqrt(.05)?\n",
    "    v2 = gammau_ir*tnu1 + (1-gammaw_il)*tnu2 #should be matrix y x h\n",
    "    \n",
    "\n",
    "#FULL CONDITIONALS BAST-RNN\n",
    "\n",
    "#W, Gamma_W\n",
    "with pm.model() as model:\n",
    "    gammaw_il = pm.Bernoulli('gammaw_il', p=0.2)\n",
    "    w_il = np.prod(math.exp(-(y_t-mu-v1*h_t+v2*(h_t)**2)*(y_t-mu-v1*h_t+v2*(h_t)**2)/(2*sigma_e2**2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAMPLING SETUP\n",
    "with model:\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(100000, step=step)\n",
    "    burned_trace = trace[250000:] #burn first 250000\n",
    "    thinned_trace = burned_trace[::5] #keep every 5th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Normal in module pymc3.distributions.continuous:\n",
      "\n",
      "class Normal(pymc3.distributions.distribution.Continuous)\n",
      " |  Normal(name, *args, **kwargs)\n",
      " |  \n",
      " |  Univariate normal log-likelihood.\n",
      " |  \n",
      " |  The pdf of this distribution is\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |     f(x \\mid \\mu, \\tau) =\n",
      " |         \\sqrt{\\frac{\\tau}{2\\pi}}\n",
      " |         \\exp\\left\\{ -\\frac{\\tau}{2} (x-\\mu)^2 \\right\\}\n",
      " |  \n",
      " |  Normal distribution can be parameterized either in terms of precision\n",
      " |  or standard deviation. The link between the two parametrizations is\n",
      " |  given by\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |     \\tau = \\dfrac{1}{\\sigma^2}\n",
      " |  \n",
      " |  .. plot::\n",
      " |  \n",
      " |      import matplotlib.pyplot as plt\n",
      " |      import numpy as np\n",
      " |      import scipy.stats as st\n",
      " |      plt.style.use('seaborn-darkgrid')\n",
      " |      x = np.linspace(-5, 5, 1000)\n",
      " |      mus = [0., 0., 0., -2.]\n",
      " |      sds = [0.4, 1., 2., 0.4]\n",
      " |      for mu, sd in zip(mus, sds):\n",
      " |          pdf = st.norm.pdf(x, mu, sd)\n",
      " |          plt.plot(x, pdf, label=r'$\\mu$ = {}, $\\sigma$ = {}'.format(mu, sd))\n",
      " |      plt.xlabel('x', fontsize=12)\n",
      " |      plt.ylabel('f(x)', fontsize=12)\n",
      " |      plt.legend(loc=1)\n",
      " |      plt.show()\n",
      " |  \n",
      " |  ========  ==========================================\n",
      " |  Support   :math:`x \\in \\mathbb{R}`\n",
      " |  Mean      :math:`\\mu`\n",
      " |  Variance  :math:`\\dfrac{1}{\\tau}` or :math:`\\sigma^2`\n",
      " |  ========  ==========================================\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  mu : float\n",
      " |      Mean.\n",
      " |  sd : float\n",
      " |      Standard deviation (sd > 0) (only required if tau is not specified).\n",
      " |  tau : float\n",
      " |      Precision (tau > 0) (only required if sd is not specified).\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  .. code-block:: python\n",
      " |  \n",
      " |      with pm.Model():\n",
      " |          x = pm.Normal('x', mu=0, sd=10)\n",
      " |  \n",
      " |      with pm.Model():\n",
      " |          x = pm.Normal('x', mu=0, tau=1/23)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Normal\n",
      " |      pymc3.distributions.distribution.Continuous\n",
      " |      pymc3.distributions.distribution.Distribution\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, mu=0, sd=None, tau=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  logcdf(self, value)\n",
      " |  \n",
      " |  logp(self, value)\n",
      " |      Calculate log-probability of Normal distribution at specified value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : numeric\n",
      " |          Value(s) for which log-probability is calculated. If the log probabilities for multiple\n",
      " |          values are desired the values must be provided in a numpy array or theano tensor\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      TensorVariable\n",
      " |  \n",
      " |  random(self, point=None, size=None)\n",
      " |      Draw random values from Normal distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      point : dict, optional\n",
      " |          Dict of variable values on which random values are to be\n",
      " |          conditioned (uses default point if not specified).\n",
      " |      size : int, optional\n",
      " |          Desired size of random sample (returns one sample if not\n",
      " |          specified).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __latex__ = _repr_latex_(self, name=None, dist=None)\n",
      " |      Magic method name for IPython to use for LaTeX formatting.\n",
      " |  \n",
      " |  default(self)\n",
      " |  \n",
      " |  get_test_val(self, val, defaults)\n",
      " |  \n",
      " |  getattr_value(self, val)\n",
      " |  \n",
      " |  logp_nojac(self, *args, **kwargs)\n",
      " |      Return the logp, but do not include a jacobian term for transforms.\n",
      " |      \n",
      " |      If we use different parametrizations for the same distribution, we\n",
      " |      need to add the determinant of the jacobian of the transformation\n",
      " |      to make sure the densities still describe the same distribution.\n",
      " |      However, MAP estimates are not invariant with respect to the\n",
      " |      parametrization, we need to exclude the jacobian terms in this case.\n",
      " |      \n",
      " |      This function should be overwritten in base classes for transformed\n",
      " |      distributions.\n",
      " |  \n",
      " |  logp_sum(self, *args, **kwargs)\n",
      " |      Return the sum of the logp values for the given observations.\n",
      " |      \n",
      " |      Subclasses can use this to improve the speed of logp evaluations\n",
      " |      if only the sum of the logp values is needed.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  dist(*args, **kwargs) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  __new__(cls, name, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pm.Normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x= np.array([1, 2, 3])\n",
    "y = np.array([5, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
